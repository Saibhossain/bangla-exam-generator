import json
from langchain_huggingface import HuggingFaceEmbeddings
from sentence_transformers import SentenceTransformer
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaLLM
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.schema import Document
from torch.nn.functional import embedding
from transformers.models.auto.image_processing_auto import model_type

with open("/Users/mdsaibhossain/code/python/Bangla Exam Generator/processed_bangla_mcqs.json","r",encoding="utf-8") as f:
    mcq_data = json.load(f)
#print(mcq_data[9])
documents = []

for item in mcq_data:
    content = f"""
        ржкрзНрж░рж╢рзНржи: {item['question']}
        ржЕржкрж╢ржи: {', '.join(item['options'])}
        рж╕ржарж┐ржХ ржЙрждрзНрждрж░: {item['answer']}
        ржмрзНржпрж╛ржЦрзНржпрж╛: {item.get('explanation', '')}
        ржмрж┐рж╖рзЯ: {item['subject']}, рж╢рзНрж░рзЗржгрж┐: {item['class']}
        """
    documents.append(Document(page_content=content, metadata=item))

#print(f" loaded {len(documents)} MCQs")

embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
vectorstore = FAISS.from_documents(documents, embedding_model)

template = """
рждрзБржорж┐ ржПржХржЬржи рж╢рж┐ржХрзНрж╖рж┐ржд рж╢рж┐ржХрзНрж╖ржХред ржирж┐ржЪрзЗ ржПржХржЯрж┐ ржкрзНрж░рж╢рзНржи, ржЕржкрж╢ржи ржУ ржЙрждрзНрждрж░ ржжрзЗржУрзЯрж╛ рж╣рзЯрзЗржЫрзЗред ржЕржирзБрж░рзВржк ржПржХржЯрж┐ ржирждрзБржи ржкрзНрж░рж╢рзНржи рждрзИрж░рж┐ ржХрж░рзЛ ржПржХржЗ ржмрж┐рж╖рзЯрзЗрж░ ржЙржкрж░, рждржмрзЗ ржмрж╛ржХрзНржп ржЧржаржи ржнрж┐ржирзНржи рж╣рждрзЗ рж╣ржмрзЗред 

рждржерзНржп:
{context}

тЬЕ ржирждрзБржи ржкрзНрж░рж╢рзНржи рждрзИрж░рж┐ ржХрж░рзЛ:
"""

prompt = PromptTemplate(
    input_variables=["context"],
    template=template
)

# ------------------ Step 4: LLM + RetrievalQA ------------------
llm = OllamaLLM(model="gemma3:1b")  # or phi, if you have it downloaded
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 2}),
    chain_type_kwargs={"prompt": prompt},
    return_source_documents=True
)

# ------------------ Step 5: Ask for Similar Question Variant ------------------
while True:
    query = input("\nЁЯФО ржкрзНрж░рж╢рзНржи ржмрж╛ ржмрж┐рж╖рзЯ рж▓рж┐ржЦрзБржи (exit рж▓рж┐ржЦрж▓рзЗ ржмржирзНржз рж╣ржмрзЗ): ")
    if query.lower() == "exit":
        break

    result = qa_chain.invoke({"query": query})
    print("\nЁЯза ржирждрзБржи ржкрзНрж░рж╢рзНржи:\n", result["result"])

    print("\nЁЯУД ржЙрзОрж╕ ржкрзНрж░рж╢рзНржи:\n", result["source_documents"][0].page_content[:300])